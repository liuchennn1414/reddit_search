{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff521aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/reddit_search/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "# Decide which dense encoding model to use \n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f4fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4775b1f-1cf2-404c-afaf-b02ed446244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv(\"/workspaces/reddit_search/data/reddit_posts_and_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46db786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine \n",
    "reddit_df['post_title_text'] = reddit_df['post_title'] + '-' + reddit_df['post_text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724d31bf-cc71-4645-b6bb-1ec024cd37b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client.delete_collection(\"reddit_post\")\n",
    "# client.delete_collection(\"reddit_comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d07f5573-cde2-4482-96aa-5c1978edb309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='reddit_post'), CollectionDescription(name='reddit_comment'), CollectionDescription(name='reddit_post_comment')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up collection \n",
    "\n",
    "# Define the collection name\n",
    "collection_name = \"reddit_post_comment\"\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=512,  # for sentence-transformers embeddings\n",
    "        distance=models.Distance.COSINE\n",
    "    ),\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "client.get_collections()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08996acb-b549-4b0a-8f97-3acad5832f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate long text \n",
    "def truncate_text(text, max_length=4000):\n",
    "    \"\"\"Truncate text but try to end at sentence boundary.\"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text\n",
    "    \n",
    "    # Truncate and try to end at sentence\n",
    "    truncated = text[:max_length]\n",
    "    last_period = truncated.rfind('.')\n",
    "    last_space = truncated.rfind(' ')\n",
    "    \n",
    "    # End at sentence if period found in last 200 chars\n",
    "    if last_period > max_length - 200:\n",
    "        return truncated[:last_period + 1]\n",
    "    # Otherwise end at word boundary\n",
    "    elif last_space > max_length - 50:\n",
    "        return truncated[:last_space]\n",
    "    else:\n",
    "        return truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d47302e5-05f6-4cf5-b377-05d88caa60ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered points: 5708\n",
      "Skipped empty: 0\n",
      "Truncated long texts: 336\n"
     ]
    }
   ],
   "source": [
    "filtered_points = []\n",
    "skipped_empty = 0\n",
    "truncated_count = 0\n",
    "id = 0\n",
    "\n",
    "for idx, row in reddit_df.iterrows():\n",
    "    # Combine title and text\n",
    "    title = str(row['post_title']) if pd.notna(row['post_title']) else \"\"\n",
    "    text = str(row['post_text']) if pd.notna(row['post_text']) else \"\"\n",
    "    comment = str(row['comment_text']) if pd.notna(row['comment_text']) else \"\"\n",
    "    combined_text = f\"{title}. {text}. {comment}\".strip(\". \")\n",
    "    \n",
    "    # Skip if essentially empty\n",
    "    if not combined_text or combined_text == \"No content\":\n",
    "        skipped_empty += 1\n",
    "        continue\n",
    "    \n",
    "    # Truncate if too long\n",
    "    original_length = len(combined_text)\n",
    "    combined_text = truncate_text(combined_text, max_length=4000)\n",
    "    \n",
    "    if len(combined_text) < original_length:\n",
    "        truncated_count += 1\n",
    "    \n",
    "    point = models.PointStruct(\n",
    "        id=id,\n",
    "        vector=models.Document(\n",
    "            text=combined_text, \n",
    "            model=model_handle\n",
    "        ),\n",
    "        payload={\n",
    "            \"text\": combined_text,\n",
    "            \"post_title\": title,\n",
    "            \"post_text\": str(row['post_text']) if pd.notna(row['post_text']) else \"\",\n",
    "            \"post_comment\": str(row['comment_text']) if pd.notna(row['comment_text']) else \"\",\n",
    "            \"subreddit\": str(row['subreddit']) if pd.notna(row['subreddit']) else \"\",\n",
    "            \"post_author\": str(row['post_author']) if pd.notna(row['post_author']) else \"\",\n",
    "            \"post_url\": str(row['post_url']) if pd.notna(row['post_url']) else \"\",\n",
    "            \"post_upvotes\": int(row['post_upvotes']) if pd.notna(row['post_upvotes']) else 0,\n",
    "            \"post_downvotes\": int(row['post_downvotes']) if pd.notna(row['post_downvotes']) else 0,\n",
    "            \"text_length\": len(combined_text),\n",
    "            \"was_truncated\": len(combined_text) < original_length,\n",
    "        }\n",
    "    )\n",
    "    filtered_points.append(point)\n",
    "    id += 1\n",
    "\n",
    "print(f\"Filtered points: {len(filtered_points)}\")\n",
    "print(f\"Skipped empty: {skipped_empty}\")\n",
    "print(f\"Truncated long texts: {truncated_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bbb03-c6f2-408a-8a2e-faba590bacf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6009e52c-d573-4282-82f6-3a3a53a59831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading 5708 points in batches of 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 229/229 [28:46<00:00,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Upload complete!\n",
      "Successful uploads: 5708\n",
      "Failed batches: 0\n",
      "Collection now has 5708 points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Upload with smaller batches and error handling\n",
    "# Memory usage is reset after each embedding call (Python releases it once the function returns and no references remain).\n",
    "# take note that you can optimize this as qdrant support batch upsert \n",
    "\n",
    "batch_size = 25  # Smaller batches\n",
    "successful_uploads = 0\n",
    "failed_batches = []\n",
    "\n",
    "print(f\"\\nUploading {len(filtered_points)} points in batches of {batch_size}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(filtered_points), batch_size)):\n",
    "    try:\n",
    "        batch = filtered_points[i:i + batch_size]\n",
    "        \n",
    "        client.upsert(\n",
    "            collection_name=\"reddit_post_comment\",\n",
    "            points=batch\n",
    "        )\n",
    "        \n",
    "        successful_uploads += len(batch)\n",
    "        \n",
    "        # Small delay to prevent overwhelming\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "        failed_batches.append(i//batch_size + 1)\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Upload complete!\")\n",
    "print(f\"Successful uploads: {successful_uploads}\")\n",
    "print(f\"Failed batches: {len(failed_batches)}\")\n",
    "\n",
    "# Verify final count\n",
    "collection_info = client.get_collection(\"reddit_post_comment\")\n",
    "print(f\"Collection now has {collection_info.points_count} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a5cb18d-b379-454b-8b96-3c9bf714df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do search \n",
    "def search(query, limit=5):\n",
    "\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name='reddit_post_comment',\n",
    "        query=models.Document( \n",
    "            text=query, # query must be text, qdrant will do the embedding for you \n",
    "            model=model_handle \n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True #to get metadata in the results\n",
    "    )\n",
    "\n",
    "    formatted_results = []\n",
    "    for point in results.points:  # Access points attribute\n",
    "        formatted_point = {\n",
    "            'post_title': point.payload['post_title'],\n",
    "            'post_text': point.payload['post_text'],\n",
    "            'subreddit': point.payload['subreddit'], \n",
    "            'post_url': point.payload['post_url'],\n",
    "            'post_upvotes': point.payload['post_upvotes'],\n",
    "            'post_comment': point.payload['post_comment']\n",
    "        }\n",
    "        formatted_results.append(formatted_point) \n",
    "\n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3549ef75-3b48-434a-988d-2811675d7925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'post_title': '[D] What are the bottlenecks holding machine learning back?',\n",
       "  'post_text': 'I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?',\n",
       "  'subreddit': 'MachineLearning',\n",
       "  'post_url': 'https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/',\n",
       "  'post_upvotes': 52,\n",
       "  'post_comment': 'All the time and attention going towards generative AI and LLMs instead of more useful things'},\n",
       " {'post_title': '[D] What are the bottlenecks holding machine learning back?',\n",
       "  'post_text': 'I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?',\n",
       "  'subreddit': 'MachineLearning',\n",
       "  'post_url': 'https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/',\n",
       "  'post_upvotes': 52,\n",
       "  'post_comment': 'Domain Knowledge driven dataset design.'},\n",
       " {'post_title': '[D] What are the bottlenecks holding machine learning back?',\n",
       "  'post_text': 'I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?',\n",
       "  'subreddit': 'MachineLearning',\n",
       "  'post_url': 'https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/',\n",
       "  'post_upvotes': 52,\n",
       "  'post_comment': 'Nuclear fusion'},\n",
       " {'post_title': '[D] What are the bottlenecks holding machine learning back?',\n",
       "  'post_text': 'I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?',\n",
       "  'subreddit': 'MachineLearning',\n",
       "  'post_url': 'https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/',\n",
       "  'post_upvotes': 52,\n",
       "  'post_comment': 'Memory bandwidth. Shuffling your 800GB language model in and out of memory every token takes more time/energy than actually doing the matrix multiplication.'},\n",
       " {'post_title': '[D] What are the bottlenecks holding machine learning back?',\n",
       "  'post_text': 'I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?',\n",
       "  'subreddit': 'MachineLearning',\n",
       "  'post_url': 'https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/',\n",
       "  'post_upvotes': 52,\n",
       "  'post_comment': 'Imo \\n- lacking understanding of the feature space. We need some big improvement on how to process data and feed it into models. Tokenizers e.g. do bit abstract, ensemble models take in raw values as well (albeit preprocessed). This kinda leads us to a linear notion of causality which is not the case in most settings. A bunch of modalities are not even understood to any degree of rigor. \\n- efficiency and densifying signals is also a large bottle neck. MOEs are cool in this regard, but again the out activation functions introduce sparsity by design, which leads to large models needed.\\n- network architectures are another big one. It is very difficult to mathematically model how the brain works. Ideally our networks should be able to revisit neurons when certain settings are given. We process everything in one direction only. \\n- math. We simply so ne have the math yet to fully understand what we are actually doing. Without the math we need to test and ablate which is neither efficient resource wise nor time wise. \\n- hype is probably the biggest bottleneck. Resource allocators have a certain view of what should be possible without understanding that is it sometimes not feasible. A lot of work has to be oversold just so the work can continue. This is not a healthy environment for innovation.\\n- benchmaxxing and paper treadmills kinda goes with the aforementioned point. Nowadays the competition is so stuff that everybody has to push more but at the same time the reviews become consistently worse and the game turned to more luck than most people would like to admit. Innovation needs to be complicated and novel enough (SimCLR got rejected by neurips for its simplicity which was the whole point, mamba was rejected 3 times) while beating all benchmarks. Industry labs have to deliver at an insane rate as well.\\n- gatekeeping is also constantly present. In order to even get into the field now, newcomers have ti be lucky already. We don’t look at promise/potential anymore but basically want ready made products. PhDs and masters admission require published papers in top tier conferences. Internships require a post doc cv, applied roles require multi year experience to start it off. Compute also hinders a lot of possibly good candidates from even entering as the previous projects need to show experience with hpc and everything at scale.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = 'What is the most trending topic about Machine Learning?'\n",
    "\n",
    "search(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae14f4-4f10-4e0c-9f06-ddd7f1fdd7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daaa74da-dbb9-4f9b-be64-af707449ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Prompt \n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a reddit summariser. Answer user's question based on the CONTEXT given to you.\n",
    "If you did not spot useful information, then answer based on your own knowledge. \n",
    "Otherwise, use only the facts from the CONTEXT when answering the question.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"title: {doc['post_title']}\\ncontent: {doc['post_text']}\\ncomment: {doc['post_comment']}\\nurl: {doc['post_url']}\\nsubreddit: {doc['subreddit']}\\npost_upvotes: {doc['post_upvotes']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efed4dfc-9106-48d2-ad09-d88660e6e9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're a reddit summariser. Answer user's question based on the CONTEXT given to you.\\nIf you did not spot useful information, then answer based on your own knowledge. \\nOtherwise, use only the facts from the CONTEXT when answering the question.\\n\\nQUESTION: What is the most trending topic about Machine Learning?\\n\\nCONTEXT: \\ntitle: [D] What are the bottlenecks holding machine learning back?\\ncontent: I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?\\ncomment: All the time and attention going towards generative AI and LLMs instead of more useful things\\nurl: https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/\\nsubreddit: MachineLearning\\npost_upvotes: 52\\n\\ntitle: [D] What are the bottlenecks holding machine learning back?\\ncontent: I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?\\ncomment: Domain Knowledge driven dataset design.\\nurl: https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/\\nsubreddit: MachineLearning\\npost_upvotes: 52\\n\\ntitle: [D] What are the bottlenecks holding machine learning back?\\ncontent: I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?\\ncomment: Nuclear fusion\\nurl: https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/\\nsubreddit: MachineLearning\\npost_upvotes: 52\\n\\ntitle: [D] What are the bottlenecks holding machine learning back?\\ncontent: I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?\\ncomment: Memory bandwidth. Shuffling your 800GB language model in and out of memory every token takes more time/energy than actually doing the matrix multiplication.\\nurl: https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/\\nsubreddit: MachineLearning\\npost_upvotes: 52\\n\\ntitle: [D] What are the bottlenecks holding machine learning back?\\ncontent: I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?\\ncomment: Imo \\n- lacking understanding of the feature space. We need some big improvement on how to process data and feed it into models. Tokenizers e.g. do bit abstract, ensemble models take in raw values as well (albeit preprocessed). This kinda leads us to a linear notion of causality which is not the case in most settings. A bunch of modalities are not even understood to any degree of rigor. \\n- efficiency and densifying signals is also a large bottle neck. MOEs are cool in this regard, but again the out activation functions introduce sparsity by design, which leads to large models needed.\\n- network architectures are another big one. It is very difficult to mathematically model how the brain works. Ideally our networks should be able to revisit neurons when certain settings are given. We process everything in one direction only. \\n- math. We simply so ne have the math yet to fully understand what we are actually doing. Without the math we need to test and ablate which is neither efficient resource wise nor time wise. \\n- hype is probably the biggest bottleneck. Resource allocators have a certain view of what should be possible without understanding that is it sometimes not feasible. A lot of work has to be oversold just so the work can continue. This is not a healthy environment for innovation.\\n- benchmaxxing and paper treadmills kinda goes with the aforementioned point. Nowadays the competition is so stuff that everybody has to push more but at the same time the reviews become consistently worse and the game turned to more luck than most people would like to admit. Innovation needs to be complicated and novel enough (SimCLR got rejected by neurips for its simplicity which was the whole point, mamba was rejected 3 times) while beating all benchmarks. Industry labs have to deliver at an insane rate as well.\\n- gatekeeping is also constantly present. In order to even get into the field now, newcomers have ti be lucky already. We don’t look at promise/potential anymore but basically want ready made products. PhDs and masters admission require published papers in top tier conferences. Internships require a post doc cv, applied roles require multi year experience to start it off. Compute also hinders a lot of possibly good candidates from even entering as the previous projects need to show experience with hpc and everything at scale.\\nurl: https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/\\nsubreddit: MachineLearning\\npost_upvotes: 52\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = 'What is the most trending topic about Machine Learning?'\n",
    "search_results = search(test_query)\n",
    "prompt = build_prompt(test_query, search_results)\n",
    "prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db3c7aa-4e93-48c8-8873-c030ffb2fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM \n",
    "import os \n",
    "import requests \n",
    "os.environ[\"API_KEY\"] = \"cannot_show_you_api_key\"\n",
    "\n",
    "def lamma3_groq(prompt):\n",
    "    api_key = os.getenv('API_KEY') \n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"llama3-8b-8192\",  # or \"llama3-70b-8192\" for larger model\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4844a30f-ebec-4074-9444-ef4e57cbb93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query): \n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = lamma3_groq(prompt)\n",
    "    return answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81ff04ff-8f37-4f7d-b641-543667e0c9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, some trending topics about machine learning that are mentioned as bottlenecks holding it back include:\\n\\n1. Generative AI and LLMs (Large Language Models) distracting from more useful applications.\\n2. Domain Knowledge driven dataset design.\\n3. Memory bandwidth and shuffling large models in and out of memory.\\n4. Lack of understanding of the feature space and processing data effectively.\\n5. Efficiency and densifying signals.\\n6. Limited understanding of network architectures and how the brain works.\\n7. Lack of mathematical understanding and the need for more rigorous testing and ablation.\\n8. Hype and overselling of work to secure funding and resources.\\n9. Benchmaxxing and paper treadmills, where reviews become consistently worse and innovation is stifled.\\n10. Gatekeeping, where newcomers face significant barriers to entry due to requirements for published papers, experience, and compute resources.\\n\\nThese topics are discussed in the comments of the post, which is over 3 years old, but the issues mentioned are still relevant in the machine learning community.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = 'What are some trending topics about machine learning?'\n",
    "rag_pipeline(test_query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4800fb9a-f5d8-43ed-acb0-a85737a8d51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, I see that some common machine learning models mentioned by Reddit users include:\\n\\n* LLMs (Large Language Models)\\n* Transformer architecture\\n* Generative AI models\\n* Federated Learning models (specifically mentioned in the comment section of the post \"P Federated Learning on a decentralized protocol (CLI demo, no central server)\")\\n\\nThese models are mentioned in various comments across different posts in the MachineLearning subreddit.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = 'What are some common machine learning models reddit users use?'\n",
    "rag_pipeline(test_query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "971de203-16c5-44b9-a39d-276c027ddd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, people in the MachineLearning subreddit are currently discussing Large Language Models (LLMs) and related topics. Specifically:\\n\\n1. In a post about the \"Favorite ML paper of 2024\", every paper discussed is related to LLMs.\\n2. In a separate post about \"The Big LLM Architecture Comparison\", users are discussing issues with token representation and vocabulary sizes in LLMs.\\n3. In another post, users are discussing \"prompt routing\" or \"model routing\", a concept that involves routing prompts to the most cost-effective LLM that can deliver a high-quality response.\\n\\nIt seems that LLMs are a dominant topic of discussion in the MachineLearning subreddit at this time.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = 'What are people discussing about LLM now?'\n",
    "rag_pipeline(test_query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a8e0f-6741-4f6e-b1a3-3b44c6839557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f5823-e470-4e17-822e-d46f7a52923b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
