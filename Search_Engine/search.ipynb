{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f704eb3",
   "metadata": {},
   "source": [
    "# 1. Init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff521aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/reddit_search/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f4fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6548bbff-afea-4304-8da3-e62b4318c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4775b1f-1cf2-404c-afaf-b02ed446244a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_author</th>\n",
       "      <th>post_url</th>\n",
       "      <th>post_upvotes</th>\n",
       "      <th>post_downvotes</th>\n",
       "      <th>comment_upvotes</th>\n",
       "      <th>comment_downvotes</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[D] - NeurIPS'2025 Reviews</td>\n",
       "      <td>Hey everyone,\\n\\nNeurIPS 2025 reviews should b...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Proof-Marsupial-5367</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>Friendly reminder that reviews this year are s...</td>\n",
       "      <td>ChoiceStranger2898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[D] - NeurIPS'2025 Reviews</td>\n",
       "      <td>Hey everyone,\\n\\nNeurIPS 2025 reviews should b...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Proof-Marsupial-5367</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>I had a dream recently where my upcoming avera...</td>\n",
       "      <td>popeldo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[D] - NeurIPS'2025 Reviews</td>\n",
       "      <td>Hey everyone,\\n\\nNeurIPS 2025 reviews should b...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Proof-Marsupial-5367</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>I will treat the scores as a divine interventi...</td>\n",
       "      <td>matcha-coconut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[D] - NeurIPS'2025 Reviews</td>\n",
       "      <td>Hey everyone,\\n\\nNeurIPS 2025 reviews should b...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Proof-Marsupial-5367</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>July 24, so as long as it's July 24 somewhere ...</td>\n",
       "      <td>SmolLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[D] - NeurIPS'2025 Reviews</td>\n",
       "      <td>Hey everyone,\\n\\nNeurIPS 2025 reviews should b...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Proof-Marsupial-5367</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>Well, if you feel heart-broken, be assured tha...</td>\n",
       "      <td>Marionberry6886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   post_title  \\\n",
       "0  [D] - NeurIPS'2025 Reviews   \n",
       "1  [D] - NeurIPS'2025 Reviews   \n",
       "2  [D] - NeurIPS'2025 Reviews   \n",
       "3  [D] - NeurIPS'2025 Reviews   \n",
       "4  [D] - NeurIPS'2025 Reviews   \n",
       "\n",
       "                                           post_text        subreddit  \\\n",
       "0  Hey everyone,\\n\\nNeurIPS 2025 reviews should b...  MachineLearning   \n",
       "1  Hey everyone,\\n\\nNeurIPS 2025 reviews should b...  MachineLearning   \n",
       "2  Hey everyone,\\n\\nNeurIPS 2025 reviews should b...  MachineLearning   \n",
       "3  Hey everyone,\\n\\nNeurIPS 2025 reviews should b...  MachineLearning   \n",
       "4  Hey everyone,\\n\\nNeurIPS 2025 reviews should b...  MachineLearning   \n",
       "\n",
       "            post_author                                           post_url  \\\n",
       "0  Proof-Marsupial-5367  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "1  Proof-Marsupial-5367  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "2  Proof-Marsupial-5367  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "3  Proof-Marsupial-5367  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "4  Proof-Marsupial-5367  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "\n",
       "   post_upvotes  post_downvotes  comment_upvotes  comment_downvotes  \\\n",
       "0           203               0               77                  0   \n",
       "1           203               0               36                  0   \n",
       "2           203               0               63                  0   \n",
       "3           203               0               33                  0   \n",
       "4           203               0               34                  0   \n",
       "\n",
       "                                        comment_text      comment_author  \n",
       "0  Friendly reminder that reviews this year are s...  ChoiceStranger2898  \n",
       "1  I had a dream recently where my upcoming avera...             popeldo  \n",
       "2  I will treat the scores as a divine interventi...      matcha-coconut  \n",
       "3  July 24, so as long as it's July 24 somewhere ...              SmolLM  \n",
       "4  Well, if you feel heart-broken, be assured tha...     Marionberry6886  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df = pd.read_csv(\"/workspaces/reddit_search/data/reddit_posts_and_comments.csv\")\n",
    "reddit_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46db786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine \n",
    "reddit_df['post_title_text'] = reddit_df['post_title'] + '-' + reddit_df['post_text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724d31bf-cc71-4645-b6bb-1ec024cd37b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_collection(\"reddit_post\")\n",
    "client.delete_collection(\"reddit_comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d07f5573-cde2-4482-96aa-5c1978edb309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='reddit_post')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up collection \n",
    "from qdrant_client import models\n",
    "\n",
    "# Define the collection name\n",
    "collection_name = \"reddit_post\"\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=512,  # for sentence-transformers embeddings\n",
    "        distance=models.Distance.COSINE\n",
    "    ),\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "client.get_collections()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0959357d-21c8-476a-8ee2-7e356fec3e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='reddit_post'), CollectionDescription(name='reddit_comment')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the collection with specified vector parameters\n",
    "\n",
    "collection_name = \"reddit_comment\"\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=512,  # for sentence-transformers embeddings\n",
    "        distance=models.Distance.COSINE\n",
    "    ),\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "client.get_collections()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08996acb-b549-4b0a-8f97-3acad5832f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate long text \n",
    "def truncate_text(text, max_length=4000):\n",
    "    \"\"\"Truncate text but try to end at sentence boundary.\"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text\n",
    "    \n",
    "    # Truncate and try to end at sentence\n",
    "    truncated = text[:max_length]\n",
    "    last_period = truncated.rfind('.')\n",
    "    last_space = truncated.rfind(' ')\n",
    "    \n",
    "    # End at sentence if period found in last 200 chars\n",
    "    if last_period > max_length - 200:\n",
    "        return truncated[:last_period + 1]\n",
    "    # Otherwise end at word boundary\n",
    "    elif last_space > max_length - 50:\n",
    "        return truncated[:last_space]\n",
    "    else:\n",
    "        return truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d47302e5-05f6-4cf5-b377-05d88caa60ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered points: 5708\n",
      "Skipped empty: 0\n",
      "Truncated long texts: 207\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "from datetime import datetime\n",
    "\n",
    "# Decide which dense encoding model to use \n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "\n",
    "\n",
    "filtered_points = []\n",
    "skipped_empty = 0\n",
    "truncated_count = 0\n",
    "id = 0\n",
    "\n",
    "for idx, row in reddit_df.iterrows():\n",
    "    # Combine title and text\n",
    "    title = str(row['post_title']) if pd.notna(row['post_title']) else \"\"\n",
    "    text = str(row['post_text']) if pd.notna(row['post_text']) else \"\"\n",
    "    combined_text = f\"{title}. {text}\".strip(\". \")\n",
    "    \n",
    "    # Skip if essentially empty\n",
    "    if not combined_text or combined_text == \"No content\":\n",
    "        skipped_empty += 1\n",
    "        continue\n",
    "    \n",
    "    # Truncate if too long\n",
    "    original_length = len(combined_text)\n",
    "    combined_text = truncate_text(combined_text, max_length=4000)\n",
    "    \n",
    "    if len(combined_text) < original_length:\n",
    "        truncated_count += 1\n",
    "    \n",
    "    point = models.PointStruct(\n",
    "        id=id,\n",
    "        vector=models.Document(\n",
    "            text=combined_text, \n",
    "            model=model_handle\n",
    "        ),\n",
    "        payload={\n",
    "            \"text\": combined_text,\n",
    "            \"post_title\": title,\n",
    "            \"post_text\": str(row['post_text']) if pd.notna(row['post_text']) else \"\",\n",
    "            \"subreddit\": str(row['subreddit']) if pd.notna(row['subreddit']) else \"\",\n",
    "            \"post_author\": str(row['post_author']) if pd.notna(row['post_author']) else \"\",\n",
    "            \"post_url\": str(row['post_url']) if pd.notna(row['post_url']) else \"\",\n",
    "            \"post_upvotes\": int(row['post_upvotes']) if pd.notna(row['post_upvotes']) else 0,\n",
    "            \"post_downvotes\": int(row['post_downvotes']) if pd.notna(row['post_downvotes']) else 0,\n",
    "            # for trend analysis \n",
    "            \"content_type\": \"post\",\n",
    "            \"engagement_score\": int(row['post_upvotes']) if pd.notna(row['post_upvotes']) else 0,\n",
    "            \"text_length\": len(combined_text),\n",
    "            \"was_truncated\": len(combined_text) < original_length,\n",
    "        }\n",
    "    )\n",
    "    filtered_points.append(point)\n",
    "    id += 1\n",
    "\n",
    "print(f\"Filtered points: {len(filtered_points)}\")\n",
    "print(f\"Skipped empty: {skipped_empty}\")\n",
    "print(f\"Truncated long texts: {truncated_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bbb03-c6f2-408a-8a2e-faba590bacf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b448fd69-35db-4dc1-8223-a42d95da74d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total points to upload: 5708\n",
      "Average text length: 1148.3 chars\n",
      "Posts with >5000 characters: 122\n"
     ]
    }
   ],
   "source": [
    "# Check how much data we're processing\n",
    "print(f\"Total points to upload: {len(points)}\")\n",
    "print(f\"Average text length: {sum(len(p.payload['text']) for p in points) / len(points):.1f} chars\")\n",
    "\n",
    "# Check for very long texts that might cause kernel to die \n",
    "long_texts = [p for p in points if len(p.payload['text']) > 5000]\n",
    "print(f\"Posts with >5000 characters: {len(long_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6009e52c-d573-4282-82f6-3a3a53a59831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading 5708 points in batches of 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 229/229 [19:09<00:00,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Upload complete!\n",
      "Successful uploads: 5708\n",
      "Failed batches: 0\n",
      "Collection now has 5708 points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Upload with smaller batches and error handling\n",
    "# Memory usage is reset after each embedding call (Python releases it once the function returns and no references remain).\n",
    "# take note that you can optimize this as qdrant support batch upsert \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "batch_size = 25  # Smaller batches\n",
    "successful_uploads = 0\n",
    "failed_batches = []\n",
    "\n",
    "print(f\"\\nUploading {len(filtered_points)} points in batches of {batch_size}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(filtered_points), batch_size)):\n",
    "    try:\n",
    "        batch = filtered_points[i:i + batch_size]\n",
    "        \n",
    "        client.upsert(\n",
    "            collection_name=\"reddit_post\",\n",
    "            points=batch\n",
    "        )\n",
    "        \n",
    "        successful_uploads += len(batch)\n",
    "        \n",
    "        # Small delay to prevent overwhelming\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "        failed_batches.append(i//batch_size + 1)\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Upload complete!\")\n",
    "print(f\"Successful uploads: {successful_uploads}\")\n",
    "print(f\"Failed batches: {len(failed_batches)}\")\n",
    "\n",
    "# Verify final count\n",
    "collection_info = client.get_collection(\"reddit_post\")\n",
    "print(f\"Collection now has {collection_info.points_count} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a5cb18d-b379-454b-8b96-3c9bf714df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do search \n",
    "def search(query, limit=1):\n",
    "\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name='reddit_post',\n",
    "        query=models.Document( \n",
    "            text=query, # query must be text, qdrant will do the embedding for you \n",
    "            model=model_handle \n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True #to get metadata in the results\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3549ef75-3b48-434a-988d-2811675d7925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResponse(points=[ScoredPoint(id=260, version=10, score=0.87068844, payload={'text': '[D] What are the bottlenecks holding machine learning back?. I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?', 'post_title': '[D] What are the bottlenecks holding machine learning back?', 'post_text': 'I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?', 'subreddit': 'MachineLearning', 'post_author': 'jacobfa', 'post_url': 'https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/', 'post_upvotes': 52, 'post_downvotes': 0, 'content_type': 'post', 'engagement_score': 52, 'text_length': 188, 'was_truncated': False}, vector=None, shard_key=None, order_value=None)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = 'What is the most trending topic about Machine Learning?'\n",
    "\n",
    "search(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a80aade-c1c3-45c3-b068-48fdb4fd2fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size=512 distance=<Distance.COSINE: 'Cosine'> hnsw_config=None quantization_config=None on_disk=None datatype=None multivector_config=None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a0b5d-d43c-4a68-95d8-02088b13e1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
