{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f704eb3",
   "metadata": {},
   "source": [
    "# 1. Init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff521aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/reddit_search/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client import models\n",
    "from datetime import datetime\n",
    "\n",
    "# Decide which dense encoding model to use \n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f4fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6548bbff-afea-4304-8da3-e62b4318c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4775b1f-1cf2-404c-afaf-b02ed446244a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_author</th>\n",
       "      <th>post_url</th>\n",
       "      <th>post_upvotes</th>\n",
       "      <th>post_downvotes</th>\n",
       "      <th>comment_upvotes</th>\n",
       "      <th>comment_downvotes</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[D] - NeurIPS'2025 Reviews</td>\n",
       "      <td>Hey everyone,\\n\\nNeurIPS 2025 reviews should b...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Proof-Marsupial-5367</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>Friendly reminder that reviews this year are s...</td>\n",
       "      <td>ChoiceStranger2898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[D] - NeurIPS'2025 Reviews</td>\n",
       "      <td>Hey everyone,\\n\\nNeurIPS 2025 reviews should b...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Proof-Marsupial-5367</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>I had a dream recently where my upcoming avera...</td>\n",
       "      <td>popeldo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[D] - NeurIPS'2025 Reviews</td>\n",
       "      <td>Hey everyone,\\n\\nNeurIPS 2025 reviews should b...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Proof-Marsupial-5367</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>I will treat the scores as a divine interventi...</td>\n",
       "      <td>matcha-coconut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[D] - NeurIPS'2025 Reviews</td>\n",
       "      <td>Hey everyone,\\n\\nNeurIPS 2025 reviews should b...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Proof-Marsupial-5367</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>July 24, so as long as it's July 24 somewhere ...</td>\n",
       "      <td>SmolLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[D] - NeurIPS'2025 Reviews</td>\n",
       "      <td>Hey everyone,\\n\\nNeurIPS 2025 reviews should b...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Proof-Marsupial-5367</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>Well, if you feel heart-broken, be assured tha...</td>\n",
       "      <td>Marionberry6886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   post_title  \\\n",
       "0  [D] - NeurIPS'2025 Reviews   \n",
       "1  [D] - NeurIPS'2025 Reviews   \n",
       "2  [D] - NeurIPS'2025 Reviews   \n",
       "3  [D] - NeurIPS'2025 Reviews   \n",
       "4  [D] - NeurIPS'2025 Reviews   \n",
       "\n",
       "                                           post_text        subreddit  \\\n",
       "0  Hey everyone,\\n\\nNeurIPS 2025 reviews should b...  MachineLearning   \n",
       "1  Hey everyone,\\n\\nNeurIPS 2025 reviews should b...  MachineLearning   \n",
       "2  Hey everyone,\\n\\nNeurIPS 2025 reviews should b...  MachineLearning   \n",
       "3  Hey everyone,\\n\\nNeurIPS 2025 reviews should b...  MachineLearning   \n",
       "4  Hey everyone,\\n\\nNeurIPS 2025 reviews should b...  MachineLearning   \n",
       "\n",
       "            post_author                                           post_url  \\\n",
       "0  Proof-Marsupial-5367  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "1  Proof-Marsupial-5367  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "2  Proof-Marsupial-5367  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "3  Proof-Marsupial-5367  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "4  Proof-Marsupial-5367  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "\n",
       "   post_upvotes  post_downvotes  comment_upvotes  comment_downvotes  \\\n",
       "0           203               0               77                  0   \n",
       "1           203               0               36                  0   \n",
       "2           203               0               63                  0   \n",
       "3           203               0               33                  0   \n",
       "4           203               0               34                  0   \n",
       "\n",
       "                                        comment_text      comment_author  \n",
       "0  Friendly reminder that reviews this year are s...  ChoiceStranger2898  \n",
       "1  I had a dream recently where my upcoming avera...             popeldo  \n",
       "2  I will treat the scores as a divine interventi...      matcha-coconut  \n",
       "3  July 24, so as long as it's July 24 somewhere ...              SmolLM  \n",
       "4  Well, if you feel heart-broken, be assured tha...     Marionberry6886  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df = pd.read_csv(\"/workspaces/reddit_search/data/reddit_posts_and_comments.csv\")\n",
    "reddit_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46db786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine \n",
    "reddit_df['post_title_text'] = reddit_df['post_title'] + '-' + reddit_df['post_text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724d31bf-cc71-4645-b6bb-1ec024cd37b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_collection(\"reddit_post\")\n",
    "client.delete_collection(\"reddit_comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d07f5573-cde2-4482-96aa-5c1978edb309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='reddit_post')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up collection \n",
    "from qdrant_client import models\n",
    "\n",
    "# Define the collection name\n",
    "collection_name = \"reddit_post\"\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=512,  # for sentence-transformers embeddings\n",
    "        distance=models.Distance.COSINE\n",
    "    ),\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "client.get_collections()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0959357d-21c8-476a-8ee2-7e356fec3e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='reddit_post'), CollectionDescription(name='reddit_comment')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the collection with specified vector parameters\n",
    "\n",
    "collection_name = \"reddit_comment\"\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=512,  # for sentence-transformers embeddings\n",
    "        distance=models.Distance.COSINE\n",
    "    ),\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "client.get_collections()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08996acb-b549-4b0a-8f97-3acad5832f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate long text \n",
    "def truncate_text(text, max_length=4000):\n",
    "    \"\"\"Truncate text but try to end at sentence boundary.\"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text\n",
    "    \n",
    "    # Truncate and try to end at sentence\n",
    "    truncated = text[:max_length]\n",
    "    last_period = truncated.rfind('.')\n",
    "    last_space = truncated.rfind(' ')\n",
    "    \n",
    "    # End at sentence if period found in last 200 chars\n",
    "    if last_period > max_length - 200:\n",
    "        return truncated[:last_period + 1]\n",
    "    # Otherwise end at word boundary\n",
    "    elif last_space > max_length - 50:\n",
    "        return truncated[:last_space]\n",
    "    else:\n",
    "        return truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d47302e5-05f6-4cf5-b377-05d88caa60ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered points: 5708\n",
      "Skipped empty: 0\n",
      "Truncated long texts: 207\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "from datetime import datetime\n",
    "\n",
    "# Decide which dense encoding model to use \n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "\n",
    "\n",
    "filtered_points = []\n",
    "skipped_empty = 0\n",
    "truncated_count = 0\n",
    "id = 0\n",
    "\n",
    "for idx, row in reddit_df.iterrows():\n",
    "    # Combine title and text\n",
    "    title = str(row['post_title']) if pd.notna(row['post_title']) else \"\"\n",
    "    text = str(row['post_text']) if pd.notna(row['post_text']) else \"\"\n",
    "    combined_text = f\"{title}. {text}\".strip(\". \")\n",
    "    \n",
    "    # Skip if essentially empty\n",
    "    if not combined_text or combined_text == \"No content\":\n",
    "        skipped_empty += 1\n",
    "        continue\n",
    "    \n",
    "    # Truncate if too long\n",
    "    original_length = len(combined_text)\n",
    "    combined_text = truncate_text(combined_text, max_length=4000)\n",
    "    \n",
    "    if len(combined_text) < original_length:\n",
    "        truncated_count += 1\n",
    "    \n",
    "    point = models.PointStruct(\n",
    "        id=id,\n",
    "        vector=models.Document(\n",
    "            text=combined_text, \n",
    "            model=model_handle\n",
    "        ),\n",
    "        payload={\n",
    "            \"text\": combined_text,\n",
    "            \"post_title\": title,\n",
    "            \"post_text\": str(row['post_text']) if pd.notna(row['post_text']) else \"\",\n",
    "            \"subreddit\": str(row['subreddit']) if pd.notna(row['subreddit']) else \"\",\n",
    "            \"post_author\": str(row['post_author']) if pd.notna(row['post_author']) else \"\",\n",
    "            \"post_url\": str(row['post_url']) if pd.notna(row['post_url']) else \"\",\n",
    "            \"post_upvotes\": int(row['post_upvotes']) if pd.notna(row['post_upvotes']) else 0,\n",
    "            \"post_downvotes\": int(row['post_downvotes']) if pd.notna(row['post_downvotes']) else 0,\n",
    "            # for trend analysis \n",
    "            \"content_type\": \"post\",\n",
    "            \"engagement_score\": int(row['post_upvotes']) if pd.notna(row['post_upvotes']) else 0,\n",
    "            \"text_length\": len(combined_text),\n",
    "            \"was_truncated\": len(combined_text) < original_length,\n",
    "        }\n",
    "    )\n",
    "    filtered_points.append(point)\n",
    "    id += 1\n",
    "\n",
    "print(f\"Filtered points: {len(filtered_points)}\")\n",
    "print(f\"Skipped empty: {skipped_empty}\")\n",
    "print(f\"Truncated long texts: {truncated_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bbb03-c6f2-408a-8a2e-faba590bacf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b448fd69-35db-4dc1-8223-a42d95da74d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total points to upload: 5708\n",
      "Average text length: 1148.3 chars\n",
      "Posts with >5000 characters: 122\n"
     ]
    }
   ],
   "source": [
    "# Check how much data we're processing\n",
    "print(f\"Total points to upload: {len(points)}\")\n",
    "print(f\"Average text length: {sum(len(p.payload['text']) for p in points) / len(points):.1f} chars\")\n",
    "\n",
    "# Check for very long texts that might cause kernel to die \n",
    "long_texts = [p for p in points if len(p.payload['text']) > 5000]\n",
    "print(f\"Posts with >5000 characters: {len(long_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6009e52c-d573-4282-82f6-3a3a53a59831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading 5708 points in batches of 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 229/229 [19:09<00:00,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Upload complete!\n",
      "Successful uploads: 5708\n",
      "Failed batches: 0\n",
      "Collection now has 5708 points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Upload with smaller batches and error handling\n",
    "# Memory usage is reset after each embedding call (Python releases it once the function returns and no references remain).\n",
    "# take note that you can optimize this as qdrant support batch upsert \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "batch_size = 25  # Smaller batches\n",
    "successful_uploads = 0\n",
    "failed_batches = []\n",
    "\n",
    "print(f\"\\nUploading {len(filtered_points)} points in batches of {batch_size}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(filtered_points), batch_size)):\n",
    "    try:\n",
    "        batch = filtered_points[i:i + batch_size]\n",
    "        \n",
    "        client.upsert(\n",
    "            collection_name=\"reddit_post\",\n",
    "            points=batch\n",
    "        )\n",
    "        \n",
    "        successful_uploads += len(batch)\n",
    "        \n",
    "        # Small delay to prevent overwhelming\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "        failed_batches.append(i//batch_size + 1)\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Upload complete!\")\n",
    "print(f\"Successful uploads: {successful_uploads}\")\n",
    "print(f\"Failed batches: {len(failed_batches)}\")\n",
    "\n",
    "# Verify final count\n",
    "collection_info = client.get_collection(\"reddit_post\")\n",
    "print(f\"Collection now has {collection_info.points_count} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a5cb18d-b379-454b-8b96-3c9bf714df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do search \n",
    "def search(query, limit=5):\n",
    "\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name='reddit_post',\n",
    "        query=models.Document( \n",
    "            text=query, # query must be text, qdrant will do the embedding for you \n",
    "            model=model_handle \n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True #to get metadata in the results\n",
    "    )\n",
    "\n",
    "    formatted_results = []\n",
    "    for point in results.points:  # Access points attribute\n",
    "        formatted_point = {\n",
    "            'post_title': point.payload['post_title'],\n",
    "            'post_text': point.payload['post_text'],\n",
    "            'subreddit': point.payload['subreddit'], \n",
    "            'post_url': point.payload['post_url'],\n",
    "            'post_upvotes': point.payload['post_upvotes']\n",
    "        }\n",
    "        formatted_results.append(formatted_point) \n",
    "\n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3549ef75-3b48-434a-988d-2811675d7925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'post_title': '[D] What are the bottlenecks holding machine learning back?',\n",
       "  'post_text': 'I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?',\n",
       "  'subreddit': 'MachineLearning',\n",
       "  'post_url': 'https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/',\n",
       "  'post_upvotes': 52},\n",
       " {'post_title': '[D] What are the bottlenecks holding machine learning back?',\n",
       "  'post_text': 'I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?',\n",
       "  'subreddit': 'MachineLearning',\n",
       "  'post_url': 'https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/',\n",
       "  'post_upvotes': 52},\n",
       " {'post_title': '[D] What are the bottlenecks holding machine learning back?',\n",
       "  'post_text': 'I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?',\n",
       "  'subreddit': 'MachineLearning',\n",
       "  'post_url': 'https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/',\n",
       "  'post_upvotes': 52},\n",
       " {'post_title': '[D] What are the bottlenecks holding machine learning back?',\n",
       "  'post_text': 'I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?',\n",
       "  'subreddit': 'MachineLearning',\n",
       "  'post_url': 'https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/',\n",
       "  'post_upvotes': 52},\n",
       " {'post_title': '[D] What are the bottlenecks holding machine learning back?',\n",
       "  'post_text': 'I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?',\n",
       "  'subreddit': 'MachineLearning',\n",
       "  'post_url': 'https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/',\n",
       "  'post_upvotes': 52}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = 'What is the most trending topic about Machine Learning?'\n",
    "\n",
    "search(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae14f4-4f10-4e0c-9f06-ddd7f1fdd7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "daaa74da-dbb9-4f9b-be64-af707449ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Prompt \n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a reddit summariser. Answer user's question based on the CONTEXT given to you.\n",
    "If you did not spot useful information, then answer based on your own knowledge. \n",
    "Otherwise, use only the facts from the CONTEXT when answering the question.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"title: {doc['post_title']}\\ncontent: {doc['post_text']}\\nurl: {doc['post_url']}\\nsubreddit: {doc['subreddit']}\\npost_upvotes: {doc['post_upvotes']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efed4dfc-9106-48d2-ad09-d88660e6e9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're a reddit summariser. Answer user's question based on the CONTEXT given to you.\\nIf you did not spot useful information, then answer based on your own knowledge. \\nOtherwise, use only the facts from the CONTEXT when answering the question.\\n\\nQUESTION: What is the most trending topic about Machine Learning?\\n\\nCONTEXT: \\ntitle: [D] What are the bottlenecks holding machine learning back?\\ncontent: I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?\\nurl: https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/\\nsubreddit: MachineLearning\\npost_upvotes: 52\\n\\ntitle: [D] What are the bottlenecks holding machine learning back?\\ncontent: I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?\\nurl: https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/\\nsubreddit: MachineLearning\\npost_upvotes: 52\\n\\ntitle: [D] What are the bottlenecks holding machine learning back?\\ncontent: I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?\\nurl: https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/\\nsubreddit: MachineLearning\\npost_upvotes: 52\\n\\ntitle: [D] What are the bottlenecks holding machine learning back?\\ncontent: I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?\\nurl: https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/\\nsubreddit: MachineLearning\\npost_upvotes: 52\\n\\ntitle: [D] What are the bottlenecks holding machine learning back?\\ncontent: I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?\\nurl: https://www.reddit.com/r/MachineLearning/comments/1lywxnm/d_what_are_the_bottlenecks_holding_machine/\\nsubreddit: MachineLearning\\npost_upvotes: 52\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = 'What is the most trending topic about Machine Learning?'\n",
    "search_results = search(test_query)\n",
    "prompt = build_prompt(test_query, formatted_results)\n",
    "prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d80c3-22ae-4925-9cc3-034cc7e551d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#curl \n",
    "#    https://api.groq.com/openai/v1/chat/completions -s \\ \n",
    "#    -H \"Content-Type: application/json\" \\ \n",
    "#    -H \"Authorization: xxxxxx\" \\ \n",
    "#    -d '{ \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \n",
    "#        \"messages\": [{ \"role\": \"user\", \n",
    "#                        \"content\": \"Explain the importance of fast language models\" }] }' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9db3c7aa-4e93-48c8-8873-c030ffb2fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM \n",
    "import os \n",
    "import requests \n",
    "os.environ[\"API_KEY\"] = \"heiheihei\"\n",
    "\n",
    "def lamma3_groq(prompt):\n",
    "    api_key = os.getenv('API_KEY') \n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"llama3-8b-8192\",  # or \"llama3-70b-8192\" for larger model\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3967dc75-b534-4dd1-94d3-68109b7499cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context, the most trending topic about Machine Learning appears to be the discussion about the bottlenecks holding machine learning back.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Answer = lamma3_groq(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4844a30f-ebec-4074-9444-ef4e57cbb93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query): \n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, formatted_results)\n",
    "    answer = lamma3_groq(prompt)\n",
    "    return answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81ff04ff-8f37-4f7d-b641-543667e0c9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context provided, it appears that there is no discussion about \"GROP\" in this thread. The title and content of the post are about the bottlenecks holding machine learning back, and there is no mention of \"GROP\" in the text.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = 'What are people discussing about GROP now?'\n",
    "rag_pipeline(test_query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4800fb9a-f5d8-43ed-acb0-a85737a8d51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the context provided, it appears that the post is about discussing the bottlenecks holding back machine learning. Since it's a discussion post, there are no specific models mentioned. However, I can provide some common machine learning models that are often discussed by Reddit users in the MachineLearning subreddit:\\n\\n1. Neural Networks: A type of machine learning model inspired by the structure and function of the human brain.\\n2. Decision Trees: A simple, tree-based model used for classification and regression tasks.\\n3. Random Forests: An ensemble learning method that combines multiple decision trees to improve accuracy.\\n4. Support Vector Machines (SVMs): A type of supervised learning algorithm that separates classes by creating a hyperplane.\\n5. Gradient Boosting: A popular ensemble learning method that combines multiple weak models to create a strong one.\\n6. K-Means: An unsupervised learning algorithm for clustering data points into groups.\\n7. Linear Regression: A type of regression analysis that models a linear relationship between variables.\\n8. Naive Bayes: A family of probabilistic classification models based on Bayes' theorem.\\n\\nThese are just a few examples, and there are many more machine learning models used in various applications.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = 'What are some common machine learning models reddit users discuss?'\n",
    "rag_pipeline(test_query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "971de203-16c5-44b9-a39d-276c027ddd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'post_title': 'Reminder: Long COVID study group today 6PM EST',\n",
       "  'post_text': \"Hello friends! A reminder that we are still doing our long COVID study group and if you want to check us out, we are having a session today at 6PM EST. Two papers will be presented by two different participants. If you want to join, let me know and I'll add you to the group chat where you will be able to access the Zoom link just before the meeting. \\n\\nRules:\\n\\n* No COVID deniers or conspiracy theorists\\n* No conversation about the origins of COVID\\n* No anti-science, anti-medicine rhetoric\\n* No anti-vaxxers\\n* No racism, sexism, ableism, homophobia, transphobia, or bigotry of any kind\\n* No harassing or bullying any other members of the group. Don’t be a jerk.\\n* Be respectful\\n* This is a study group and not a support group. This time will be spent discussing scientific research. If you are looking for emotional support, or a place to vent, I recommend trying to find long covid support groups.\\n* Presenters can use ChatGPT to help them understand the article but all presenters are expected to read the entirety of the article.\\n\\n  \\nDisclaimer/accessibility:\\n\\n* This study group is for educational and informational purposes only. Nothing discussed here should be considered medical advice. Always consult with a qualified healthcare professional before making any decisions regarding your health, treatment, or medical care.\\n* Content warning: this study group will discuss the health impacts of long COVID. This can be emotionally distressing to some people.\\xa0\\n* Accessibility: This study group will involved time looking at screens, listening to presenters speak (prolonged audio). Presenters will be required to read complex articles, explain them, and to speak about them. There will be breaks throughout the session. I will try to always record sessions for folks as well. Please be mindful of your limits before attending a session or before volunteering to present.\",\n",
       "  'subreddit': 'covidlonghaulers',\n",
       "  'post_url': 'https://www.reddit.com/r/covidlonghaulers/comments/1m6pssd/reminder_long_covid_study_group_today_6pm_est/',\n",
       "  'post_upvotes': 18},\n",
       " {'post_title': 'Reminder: Long COVID study group today 6PM EST',\n",
       "  'post_text': \"Hello friends! A reminder that we are still doing our long COVID study group and if you want to check us out, we are having a session today at 6PM EST. Two papers will be presented by two different participants. If you want to join, let me know and I'll add you to the group chat where you will be able to access the Zoom link just before the meeting. \\n\\nRules:\\n\\n* No COVID deniers or conspiracy theorists\\n* No conversation about the origins of COVID\\n* No anti-science, anti-medicine rhetoric\\n* No anti-vaxxers\\n* No racism, sexism, ableism, homophobia, transphobia, or bigotry of any kind\\n* No harassing or bullying any other members of the group. Don’t be a jerk.\\n* Be respectful\\n* This is a study group and not a support group. This time will be spent discussing scientific research. If you are looking for emotional support, or a place to vent, I recommend trying to find long covid support groups.\\n* Presenters can use ChatGPT to help them understand the article but all presenters are expected to read the entirety of the article.\\n\\n  \\nDisclaimer/accessibility:\\n\\n* This study group is for educational and informational purposes only. Nothing discussed here should be considered medical advice. Always consult with a qualified healthcare professional before making any decisions regarding your health, treatment, or medical care.\\n* Content warning: this study group will discuss the health impacts of long COVID. This can be emotionally distressing to some people.\\xa0\\n* Accessibility: This study group will involved time looking at screens, listening to presenters speak (prolonged audio). Presenters will be required to read complex articles, explain them, and to speak about them. There will be breaks throughout the session. I will try to always record sessions for folks as well. Please be mindful of your limits before attending a session or before volunteering to present.\",\n",
       "  'subreddit': 'covidlonghaulers',\n",
       "  'post_url': 'https://www.reddit.com/r/covidlonghaulers/comments/1m6pssd/reminder_long_covid_study_group_today_6pm_est/',\n",
       "  'post_upvotes': 18},\n",
       " {'post_title': 'Reminder: Long COVID study group today 6PM EST',\n",
       "  'post_text': \"Hello friends! A reminder that we are still doing our long COVID study group and if you want to check us out, we are having a session today at 6PM EST. Two papers will be presented by two different participants. If you want to join, let me know and I'll add you to the group chat where you will be able to access the Zoom link just before the meeting. \\n\\nRules:\\n\\n* No COVID deniers or conspiracy theorists\\n* No conversation about the origins of COVID\\n* No anti-science, anti-medicine rhetoric\\n* No anti-vaxxers\\n* No racism, sexism, ableism, homophobia, transphobia, or bigotry of any kind\\n* No harassing or bullying any other members of the group. Don’t be a jerk.\\n* Be respectful\\n* This is a study group and not a support group. This time will be spent discussing scientific research. If you are looking for emotional support, or a place to vent, I recommend trying to find long covid support groups.\\n* Presenters can use ChatGPT to help them understand the article but all presenters are expected to read the entirety of the article.\\n\\n  \\nDisclaimer/accessibility:\\n\\n* This study group is for educational and informational purposes only. Nothing discussed here should be considered medical advice. Always consult with a qualified healthcare professional before making any decisions regarding your health, treatment, or medical care.\\n* Content warning: this study group will discuss the health impacts of long COVID. This can be emotionally distressing to some people.\\xa0\\n* Accessibility: This study group will involved time looking at screens, listening to presenters speak (prolonged audio). Presenters will be required to read complex articles, explain them, and to speak about them. There will be breaks throughout the session. I will try to always record sessions for folks as well. Please be mindful of your limits before attending a session or before volunteering to present.\",\n",
       "  'subreddit': 'covidlonghaulers',\n",
       "  'post_url': 'https://www.reddit.com/r/covidlonghaulers/comments/1m6pssd/reminder_long_covid_study_group_today_6pm_est/',\n",
       "  'post_upvotes': 18},\n",
       " {'post_title': 'Reminder: Long COVID study group today 6PM EST',\n",
       "  'post_text': \"Hello friends! A reminder that we are still doing our long COVID study group and if you want to check us out, we are having a session today at 6PM EST. Two papers will be presented by two different participants. If you want to join, let me know and I'll add you to the group chat where you will be able to access the Zoom link just before the meeting. \\n\\nRules:\\n\\n* No COVID deniers or conspiracy theorists\\n* No conversation about the origins of COVID\\n* No anti-science, anti-medicine rhetoric\\n* No anti-vaxxers\\n* No racism, sexism, ableism, homophobia, transphobia, or bigotry of any kind\\n* No harassing or bullying any other members of the group. Don’t be a jerk.\\n* Be respectful\\n* This is a study group and not a support group. This time will be spent discussing scientific research. If you are looking for emotional support, or a place to vent, I recommend trying to find long covid support groups.\\n* Presenters can use ChatGPT to help them understand the article but all presenters are expected to read the entirety of the article.\\n\\n  \\nDisclaimer/accessibility:\\n\\n* This study group is for educational and informational purposes only. Nothing discussed here should be considered medical advice. Always consult with a qualified healthcare professional before making any decisions regarding your health, treatment, or medical care.\\n* Content warning: this study group will discuss the health impacts of long COVID. This can be emotionally distressing to some people.\\xa0\\n* Accessibility: This study group will involved time looking at screens, listening to presenters speak (prolonged audio). Presenters will be required to read complex articles, explain them, and to speak about them. There will be breaks throughout the session. I will try to always record sessions for folks as well. Please be mindful of your limits before attending a session or before volunteering to present.\",\n",
       "  'subreddit': 'covidlonghaulers',\n",
       "  'post_url': 'https://www.reddit.com/r/covidlonghaulers/comments/1m6pssd/reminder_long_covid_study_group_today_6pm_est/',\n",
       "  'post_upvotes': 18},\n",
       " {'post_title': 'Reminder: Long COVID study group today 6PM EST',\n",
       "  'post_text': \"Hello friends! A reminder that we are still doing our long COVID study group and if you want to check us out, we are having a session today at 6PM EST. Two papers will be presented by two different participants. If you want to join, let me know and I'll add you to the group chat where you will be able to access the Zoom link just before the meeting. \\n\\nRules:\\n\\n* No COVID deniers or conspiracy theorists\\n* No conversation about the origins of COVID\\n* No anti-science, anti-medicine rhetoric\\n* No anti-vaxxers\\n* No racism, sexism, ableism, homophobia, transphobia, or bigotry of any kind\\n* No harassing or bullying any other members of the group. Don’t be a jerk.\\n* Be respectful\\n* This is a study group and not a support group. This time will be spent discussing scientific research. If you are looking for emotional support, or a place to vent, I recommend trying to find long covid support groups.\\n* Presenters can use ChatGPT to help them understand the article but all presenters are expected to read the entirety of the article.\\n\\n  \\nDisclaimer/accessibility:\\n\\n* This study group is for educational and informational purposes only. Nothing discussed here should be considered medical advice. Always consult with a qualified healthcare professional before making any decisions regarding your health, treatment, or medical care.\\n* Content warning: this study group will discuss the health impacts of long COVID. This can be emotionally distressing to some people.\\xa0\\n* Accessibility: This study group will involved time looking at screens, listening to presenters speak (prolonged audio). Presenters will be required to read complex articles, explain them, and to speak about them. There will be breaks throughout the session. I will try to always record sessions for folks as well. Please be mindful of your limits before attending a session or before volunteering to present.\",\n",
       "  'subreddit': 'covidlonghaulers',\n",
       "  'post_url': 'https://www.reddit.com/r/covidlonghaulers/comments/1m6pssd/reminder_long_covid_study_group_today_6pm_est/',\n",
       "  'post_upvotes': 18}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = 'What are people discussing about GROP now?'\n",
    "search_results = search(test_query)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a8e0f-6741-4f6e-b1a3-3b44c6839557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f5823-e470-4e17-822e-d46f7a52923b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
